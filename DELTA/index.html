<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DELTA: Dense Depth from Events and LiDAR using Transformer's Attention</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4Q6Gf2aSP4eDXB8Miphtr37CMZZQ5oXLH2yaXMJ2w8e2ZtHTl7GptT4jmndRuHDT" crossorigin="anonymous">
    <link rel="stylesheet" href="../resources/style.css">
    <script type="text/javascript" src="../resources/colormode.js"></script>
  </head>
  <body>
    <section class="bg-body-secondary">
      <div class="container text-center p-4">
        <h1>DELTA: Dense Depth from Events and LiDAR using Transformer's Attention</h1>
        <h3 class="text-secondary">CVPR Workshop on Event-based Vision, Jun. 2025</h3>
        <hr>
        <p>
          <a href="..">Vincent Brebion</a><sup>1,2,3</sup>,
          <a href="https://www.hds.utc.fr/~moreajul">Julien Moreau</a><sup>2</sup>,
          <a href="https://davoinef.github.io">Franck Davoine</a><sup>3</sup> <br>
          <sup>1</sup>Centre for Environmental and Climate Science, Lund University, Sweden<br>
          <sup>2</sup>Université de technologie de Compiègne, CNRS, Heudiasyc, France<br>
          <sup>3</sup>CNRS, INSA Lyon, UCBL, LIRIS, UMR5205, France
        </p>
        <div class="all-links">
          <a href="article/DELTA: Dense Depth from Events and LiDAR using Transformer's Attention.pdf" target="_blank" rel="noopener noreferrer" class="btn btn-primary" role="button"><img src="../resources/logos/document.svg" height="30"/ class="my-1"><br>Article (incl. suppl. mat.)</a>
          <a href="https://openaccess.thecvf.com/content/CVPR2025W/EventVision/papers/Brebion_DELTA_Dense_Depth_from_Events_and_LiDAR_using_Transformers_Attention_CVPRW_2025_paper.pdf" target="_blank" rel="noopener noreferrer" class="btn btn-primary" role="button"><img src="../resources/logos/document.svg" height="30"/ class="my-1"><br>Article (CVF)</a>
          <a href="https://openaccess.thecvf.com/content/CVPR2025W/EventVision/supplemental/Brebion_DELTA_Dense_Depth_CVPRW_2025_supplemental.pdf" target="_blank" rel="noopener noreferrer" class="btn btn-primary" role="button"><img src="../resources/logos/document.svg" height="30"/ class="my-1"><br>Suppl. mat. (CVF)</a>
          <a href="poster/poster_delta.pdf" target="_blank" rel="noopener noreferrer" class="btn btn-warning" role="button"><img src="../resources/logos/poster.svg" height="30"/ class="my-1"><br>Poster</a>
          <a href="https://github.com/heudiasyc/DELTA" target="_blank" rel="noopener noreferrer" class="btn btn-dark" role="button"><img src="../resources/logos/github.svg" height="30"/ class="my-1"><br>Source code</a>
        </div>
      </div>
    </section>
    <section>
      <div class="container p-4">
        <video src="videos/delta.webm" width="100%" autoplay muted loop></video>
        <hr class="my-4">
        <h5>Abstract</h5>
        <p>Event cameras and LiDARs provide complementary yet distinct data: respectively, asynchronous detections of changes in lighting versus sparse but accurate depth information at a fixed rate. To this day, few works have explored the combination of these two modalities. In this article, we propose a novel neural-network-based method for fusing event and LiDAR data in order to estimate dense depth maps. Our architecture, DELTA, exploits the concepts of self- and cross-attention to model the spatial and temporal relations within and between the event and LiDAR data. Following a thorough evaluation, we demonstrate that DELTA sets a new state of the art in the event-based depth estimation problem, and that it is able to reduce the errors up to four times for close ranges compared to the previous SOTA.</p>
        <hr class="my-4">
        <h5>Citation</h5>
        <pre class="bg-body-secondary px-3 py-3"><code>@article{Brebion2025DELTADD,
<!--   -->  title={{DELTA}: Dense Depth from Events and {LiDAR} using Transformer's Attention},
<!--   -->  author={Vincent Brebion and Julien Moreau and Franck Davoine},
<!--   -->  journal={Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops},
<!--   -->  pages={4898-4907},
<!--   -->  year={2025}
<!--   -->}</code></pre>
        <hr class="my-4">
        <h5>Acknowledgment</h5>
        <p>
          This work was supported in part by the Hauts-de-France Region and in part by the SIVALab Joint Laboratory (Renault Group - Université de technologie de Compiègne (UTC) - Centre National de la Recherche Scientifique (CNRS)).<br>
          This work was also supported by The Royal Physiographic Society in Lund.
        </p>
      </div>
    </section>
  </body>
</html>
